[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "This blog is for people just starting out with their R journey. Iâ€™ll address beginner problems, simplifying some concepts in R programming.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with OSM history files in R\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nScraping wikipedia\n\n\n\ncode\n\nsummary\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\nSarah Listabarth\n\n\n\n\n\n\n\n\n\n\n\n\nSending automated emails\n\n\n\ncode\n\nvisualization\n\n\n\n\n\n\n\n\n\nFeb 12, 2025\n\n\nSarah Listabarth\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a locator map with ggplot2\n\n\n\ncode\n\nvisualization\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nSarah Listabarth\n\n\n\n\n\n\n\n\n\n\n\n\nOpening .kmz files\n\n\n\ncode\n\nspatial formats\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nSarah Listabarth\n\n\n\n\n\n\n\n\n\n\n\n\nReading a csv file with 2-row column names\n\n\n\ncode\n\ndata\n\n\n\nSometimes, csv files have 2-row column names. I create a function to deal with this.\n\n\n\n\n\nMay 5, 2024\n\n\nSarah Listabarth\n\n\n\n\n\n\n\n\n\n\n\n\nOpening .nc files\n\n\n\ncode\n\nspatial formats\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nSarah Listabarth\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a map with ggplot2\n\n\n\ncode\n\nvisualization\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nSarah Listabarth\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a basic regression table with modelsummary\n\n\n\ncode\n\nsummary\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nSarah Listabarth\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/opening-nc-files/index.html",
    "href": "blog/posts/opening-nc-files/index.html",
    "title": "Opening .nc files",
    "section": "",
    "text": "When working with raster data, one thing Iâ€™ve noticed is: thereâ€™s a lot of obscure data formats. And with every one of them, it takes me a while to figure out how to handle them in R. So I decided to make a series of blog posts on how to handle some of the raster formats Iâ€™ve come across.\nThe first one are .nc files, also known as netCDF. From what Iâ€™ve learned, they usually have three layers:\nHowever, the order of these three layers varies, which means youâ€™ll have to get acquainted with your data first. So:"
  },
  {
    "objectID": "blog/posts/opening-nc-files/index.html#open-nc-file",
    "href": "blog/posts/opening-nc-files/index.html#open-nc-file",
    "title": "Opening .nc files",
    "section": "Open nc file",
    "text": "Open nc file\nThe first thing we do is have a look at the variable names (theyâ€™re saved under var in the weather list). Letâ€™s also check out their names, which is saved under longname in this ncd4 format. Additionally, letâ€™s find out their respective dimensions, saved under size.\n\nraster &lt;- raster_path |&gt; nc_open()\n\nraster |&gt; \n  pluck(\"var\") |&gt; \n  map_df(~ .x[c(\"longname\", \"size\")] |&gt; as.character()) |&gt; \n  # you could stop here, but I wanted a nice display\n  t() |&gt; \n  data.frame() |&gt; \n  tibble::rownames_to_column() |&gt; \n  setNames(c(\"variable\", \"name\", \"dimension\")) |&gt; \n  kableExtra::kbl()\n\n\n\n\nvariable\nname\ndimension\n\n\n\n\ntime_bnds\ntime_bnds\nc(2, 365)\n\n\nlon\nLongitude Of Cell Center\nc(240, 220)\n\n\nlat\nLatitude Of Cell Center\nc(240, 220)\n\n\nx_bnds\nx_bnds\nc(2, 240)\n\n\ny_bnds\ny_bnds\nc(2, 220)\n\n\ncrs_HYRAS\nDWD HYRAS ETRS89 LCC grid with 240 columns and 220 rows\n1\n\n\ntas\nDaily Mean Air Temperature\nc(240, 220, 365)\n\n\nnumber_of_stations\nNumber Of Stations Available For Interpolation Per Day All Over The HYRAS Area\n365"
  },
  {
    "objectID": "blog/posts/opening-nc-files/index.html#sec-vars",
    "href": "blog/posts/opening-nc-files/index.html#sec-vars",
    "title": "Opening .nc files",
    "section": "Time and spatial layers",
    "text": "Time and spatial layers\nWeâ€™re definitely going to need something along the lines of latitude and longitude. From the descriptions, we can see that lat and lon describe the cell center, and their dimension is 240 \\(\\times\\) 220. That is not what we need. Instead, we want something with dimensions of 1â€“2 \\(\\times\\) 240 or 220, which describes the latitude and longitude in general, not for every cell center. In this case, that applies for x_bnds and y_bnds.\nWe also need something specifying the time. In this case, that is time_bnds.\nLetâ€™s also get the crs for good measure, crs_HYRAS. This only has dimension 1, so we need to extract is as an attribute.\nAll of these layers can be named differently in different files, so it pays off to check out the specific name. Letâ€™s save them into variables so we canâ€™t forget them!\n\ntime &lt;- ncvar_get(raster, \"time_bnds\")\nlon &lt;- ncvar_get(raster, \"x_bnds\")\nlat &lt;- ncvar_get(raster, \"y_bnds\")\ncrs &lt;- ncatt_get(raster, \"crs_HYRAS\")$epsg_code"
  },
  {
    "objectID": "blog/posts/opening-nc-files/index.html#variable-layer",
    "href": "blog/posts/opening-nc-files/index.html#variable-layer",
    "title": "Opening .nc files",
    "section": "Variable layer",
    "text": "Variable layer\nNow, we also need the actual variable weâ€™re looking for. In this case, itâ€™s tas (for mean daily temperature). We can see that the dimensions are the largest and match our geospatial and time dimensions: 240 (x_bnds) \\(\\times\\) 220 (y_bnds) \\(\\times\\) 365 (time_bnds).\nLetâ€™s get this variableâ€™s array out. Additionally, letâ€™s find out how the NAs are coded, and use that information to code them as NAs that R recognizes.\n\nvariable_array &lt;- ncvar_get(raster, \"tas\")\nfillvalue &lt;- ncatt_get(raster, \"tas\", \"_FillValue\")\n\n# set NA value\nvariable_array[variable_array == fillvalue$value] &lt;- NA"
  },
  {
    "objectID": "blog/posts/opening-nc-files/index.html#close-nc-file",
    "href": "blog/posts/opening-nc-files/index.html#close-nc-file",
    "title": "Opening .nc files",
    "section": "Close nc file",
    "text": "Close nc file\nNow we have all the information we need, yay! Letâ€™s not forget to close the .nc file again.\n\nnc_close(raster)"
  },
  {
    "objectID": "blog/posts/opening-nc-files/index.html#making-a-raster",
    "href": "blog/posts/opening-nc-files/index.html#making-a-raster",
    "title": "Opening .nc files",
    "section": "Making a raster",
    "text": "Making a raster\nNext, we make a raster of this with the terra package. We already know the structure of the array, where time is the last layer. This does vary over different files though!\nWe declare the extent and the crs that we extracted in SectionÂ 2.2. Then, letâ€™s go ahead and plot it!\n\ngot_weather_array &lt;- variable_array[,, day_of_year]\ngot_weather_raster &lt;- got_weather_array |&gt; \n  terra::rast(extent = ext(min(lon),\n                             max(lon),\n                             min(lat),\n                             max(lat)),\n                crs = crs)\nplot(got_weather_raster)\n\n\n\n\n\n\n\n\nWell, this somewhat looks like Germany â€¦ but not quite yet. We need to mirror it and turn it by 90Â°."
  },
  {
    "objectID": "blog/posts/opening-nc-files/index.html#transposing-the-raster",
    "href": "blog/posts/opening-nc-files/index.html#transposing-the-raster",
    "title": "Opening .nc files",
    "section": "Transposing the raster",
    "text": "Transposing the raster\nFor this, we need to go back to the last step, and transpose the 2-dimensional array for our specific day. Then, we make it a raster again.\n\ngot_weather_raster_transposed &lt;- got_weather_array |&gt;\n  t() |&gt;\n  terra::rast(extent = ext(min(lon),\n                           max(lon),\n                           min(lat),\n                           max(lat)),\n              crs = crs)\n\ngot_weather_raster_transposed |&gt; \n  plot()\n\n\n\n\n\n\n\n\nWell, this is almost right â€“ we just need to turn it upside down now."
  },
  {
    "objectID": "blog/posts/opening-nc-files/index.html#flipping-the-raster",
    "href": "blog/posts/opening-nc-files/index.html#flipping-the-raster",
    "title": "Opening .nc files",
    "section": "Flipping the raster",
    "text": "Flipping the raster\nFor this task, terra has a specific function, flip. We can say which way to flip the raster â€“ in this case, vertically. Letâ€™s go ahead and plot this again.\n\ngot_weather_raster_right_side_up &lt;- got_weather_raster_transposed |&gt; \n  flip(direction = \"vertical\")\n\ngot_weather_raster_right_side_up |&gt; plot()\n\n\n\n\n\n\n\n\nAnd there we have it!"
  },
  {
    "objectID": "blog/posts/creating-a-basic-map/index.html",
    "href": "blog/posts/creating-a-basic-map/index.html",
    "title": "Creating a map with ggplot2",
    "section": "",
    "text": "In this post, I create a basic world map â€“ specifically, a Choropleth map. That means we colour the countries by a specific variable."
  },
  {
    "objectID": "blog/posts/creating-a-basic-map/index.html#basic-map",
    "href": "blog/posts/creating-a-basic-map/index.html#basic-map",
    "title": "Creating a map with ggplot2",
    "section": "Basic map",
    "text": "Basic map\nWe can simply do this with ggplot, with the function geom_sf. It takes normal aesthetics, so we can just hand it our variable of interest â€“ is_my_country. Since this is a map, it makes the most sense to just fill the polygons according to this variable, so we use the fill aesthetic.\n\nggplot() +\n  # plot an sf object\n  geom_sf(data = world,\n          # fill it according to my variable\n          aes(fill = is_my_country))\n\n\n\n\n\n\n\nFigureÂ 1: Choropleth map of my countries â€“ basic"
  },
  {
    "objectID": "blog/posts/creating-a-basic-map/index.html#intermediate-map",
    "href": "blog/posts/creating-a-basic-map/index.html#intermediate-map",
    "title": "Creating a map with ggplot2",
    "section": "Intermediate map",
    "text": "Intermediate map\nWe now decrease some of the clutter. We get rid of the legend since itâ€™s just a Boolean â€“ we can indicate this in our title/caption. We also choose different colours, and get rid of the gridlines.\n\nggplot() +\n  # plot an sf object\n  geom_sf(data = world,\n          # fill it according to my variable\n          aes(fill = is_my_country),\n         # don't show the legend: it's just true or false, can be shown in title\n          show.legend = FALSE) +\n  # make colours prettier\n  scale_fill_manual(values = c(\"white\", \"wheat\")) +\n  # remove clutter\n  theme_void() \n\n\n\n\n\n\n\nFigureÂ 2: Choropleth map of my countries (in yellow) â€“ intermediate"
  },
  {
    "objectID": "blog/posts/creating-a-basic-map/index.html#prettier-map",
    "href": "blog/posts/creating-a-basic-map/index.html#prettier-map",
    "title": "Creating a map with ggplot2",
    "section": "Prettier map",
    "text": "Prettier map\nIt doesnâ€™t quite look like weâ€™re used to, though. Check out the comments to see what weâ€™ve changed.\n\nggplot() +\n  # plot an sf object\n  geom_sf(\n    data = world,\n    # fill it according to my variable\n    aes(fill = is_my_country),\n    # make borders lighter\n    col = \"grey80\",\n    # don't show the legend: it's just true or false, can be shown in title\n    show.legend = FALSE\n  ) +\n  # add country labels\n  geom_sf_text(\n    # get the data just for the countries we want to show\n    data = world |&gt; filter(is_my_country == TRUE),\n    # get the sovereignt label, and transform it to upper case\n    aes(label = admin |&gt; toupper()),\n    # make it not as dark\n    col = \"grey30\",\n    # decrease size\n    size = 2.5\n  ) +\n  # make colours prettier\n  scale_fill_manual(values = c(\"white\", \"wheat\")) +\n  # change to a nicer projection: equal area (more accurate)\n  coord_sf(crs = \"ESRI:54009\") +\n  # remove clutter\n  theme_void()\n\n\n\n\n\n\n\nFigureÂ 3: Choropleth map of my countries â€“ prettier version"
  },
  {
    "objectID": "blog/posts/creating-a-basic-map/index.html#prettier-map-with-ocean",
    "href": "blog/posts/creating-a-basic-map/index.html#prettier-map-with-ocean",
    "title": "Creating a map with ggplot2",
    "section": "Prettier map with ocean",
    "text": "Prettier map with ocean\nThereâ€™s just some lines of code you need to add to have a round earth/rounded sea. We need to create a polygon that has just the shape of the earth. We can do this with st_graticule, and then st_cast it to a polygon. Then, we can simply plot this polygon at the beginning of our ggplot.\n\ngrat &lt;- st_graticule() |&gt; st_cast('POLYGON')\n\nggplot() +\n  # this is the new line\n  geom_sf(data = grat, fill = \"#d7ecfa\", col = \"#d7ecfa\") +\n  # now everything is the same as before\n  geom_sf(\n    data = world,\n    aes(fill = is_my_country),\n    col = \"grey80\",\n    show.legend = FALSE\n  ) +\n  # add country labels\n  geom_sf_text(\n    data = world |&gt; filter(is_my_country == TRUE),\n    aes(label = admin |&gt; toupper()),\n    col = \"grey30\",\n    size = 2.5\n  ) +\n  # make colours prettier\n  scale_fill_manual(values = c(\"white\", \"wheat\")) +\n  # change to a nicer projection: equal area (more accurate)\n  coord_sf(crs = \"ESRI:54009\") +\n  # remove clutter\n  theme_void()\n\n\n\n\n\n\n\nFigureÂ 4: Choropleth map of my countries â€“ round earth"
  },
  {
    "objectID": "blog/posts/creating-a-basic-map/index.html#advanced-stuff",
    "href": "blog/posts/creating-a-basic-map/index.html#advanced-stuff",
    "title": "Creating a map with ggplot2",
    "section": "Advanced stuff",
    "text": "Advanced stuff\nIf youâ€™re really interested, you can check out the following on top:\n\ngraticules (latitude/longitude)\nNorth arrow (not recommended for world maps, though)\nscale (not recommended for most world maps, though)"
  },
  {
    "objectID": "blog/posts/showing-regression-results/index.html",
    "href": "blog/posts/showing-regression-results/index.html",
    "title": "Creating a basic regression table with modelsummary",
    "section": "",
    "text": "In this post, we have a look at creating a basic regression table with modelsummary and fixest."
  },
  {
    "objectID": "blog/posts/showing-regression-results/index.html#basic-table",
    "href": "blog/posts/showing-regression-results/index.html#basic-table",
    "title": "Creating a basic regression table with modelsummary",
    "section": "Basic table",
    "text": "Basic table\nCreating a summary table of our equations is very straight-forward with modelsummary: We create a list of the models we want to show, and then input that to modelsummary.\n\nlist(gravity_ols,\n     gravity_pois) |&gt;\n  modelsummary()\n\n\n\nTableÂ 1: Some regressions â€“ basic\n\n\n\n\n\n\n\nÂ (1)\nÂ Â (2)\n\n\n\n\nlog_dist_km\n-47643.024\n-0.002\n\n\n\n(10905.572)\n(0.000)\n\n\nNum.Obs.\n38325\n38325\n\n\nR2\n0.286\n0.751\n\n\nR2 Adj.\n0.285\n0.751\n\n\nR2 Within\n0.031\n0.269\n\n\nR2 Within Adj.\n0.031\n0.269\n\n\nAIC\n1533832.6\n1e+12\n\n\nBIC\n1534328.7\n1e+12\n\n\nRMSE\n118498460.83\n88327120.62\n\n\nStd.Errors\nby: Origin\nby: Origin\n\n\nFE: Origin\nX\nX\n\n\nFE: Destination\nX\nX\n\n\nFE: Product\nX\nX\n\n\nFE: Year\nX\nX"
  },
  {
    "objectID": "blog/posts/showing-regression-results/index.html#prettier-table",
    "href": "blog/posts/showing-regression-results/index.html#prettier-table",
    "title": "Creating a basic regression table with modelsummary",
    "section": "Prettier table",
    "text": "Prettier table\nHowever, the output in TableÂ 1 is not very pretty yet. Itâ€™s not entirely clear yet what the independent variable is, we donâ€™t know what (1) and (2) stand for, and we have a mass of goodness-of-fit measures. Letâ€™s customize our table!\nAs a first step, we make create some helper functions. These help with formatting the table.\n\n\nCode\n# format numbers: thousand separator\nf &lt;- function(x, n_digits = 2) {\n  ifelse(is.na(x),\n         \"\",\n         formatC(\n           x,\n           digits = n_digits,\n           big.mark = \",\",\n           format = \"f\"\n         ))\n}\n\nf_0 &lt;- purrr::partial(f, n_digits = 0)\n  \n#  function for GOF measures we don't want to change\nkeep_format &lt;- function(x) list(\"raw\" = x, \"clean\" = x, \"fmt\" = NA)\n\n\nThen, we create a list where we format our goodness-of-fit (GOF) measures. Some of the default names are not so pretty, e.g.Â Num.Observations without a space between the two words â€“ so we switch them to shorter or nicer names.\n\n\nCode\n# format # observations and R^2, keep the rest\ngof_tidy &lt;- list(\n  list(\n    \"raw\" = \"nobs\",\n    \"clean\" = \"Observations\",\n    \"fmt\" = f_0\n  ),\n  list(\n    \"raw\" = \"r.squared\",\n    \"clean\" = \"R\\u00B2\",\n    \"fmt\" = 3\n  ),\n  keep_format(\"FE: Origin\"),\n  keep_format(\"FE: Destination\"),\n  keep_format(\"FE: Product\"),\n  keep_format(\"FE: Year\")\n)\n\n\nLetâ€™s change the labels for our regression. We do this by adding names to the listâ€™s input (lines 1â€“2).\nAs a next step, letâ€™s use the label we added earlier on, by setting coef_rename to true. Letâ€™s also format the numbers using the formatting function we set up earlier, f.\nLetâ€™s omit some of the goodnes-of-fit (gof) indicators, since we donâ€™t need all of them here. We do this with the gof_map argument, to which we supply our GOF list from the last step. Alternatively, we could use a regex in the gof_omit argument: anything that matches the expression in line 4 will not be included.\nAlso, Iâ€™m used to adding stars where a coefficient is significant. This is not added by default, so letâ€™s simply set the stars argument to true.\nThen, weâ€™re setting the output to gt, which gives us the possibility to further style the table with the package gt. We add a header detailing our dependent variable. Then, we add a spanner to tell readers that OLS and Poisson are regression models.\n\nlist(OLS = gravity_ols,\n     Poisson = gravity_pois) |&gt;\n  modelsummary(\n    coef_rename = TRUE,\n    gof_map = gof_tidy,\n    fmt = f,\n    stars = TRUE,\n    output = \"gt\"\n  ) |&gt;\n  # add header and spanner\n  tab_header(title = \"Dependent variable: Trade flow [â‚¬]\") |&gt;\n  tab_spanner(\n    label = \"Regression model\",\n    columns = c(\"OLS\", \"Poisson\")\n    )\n\n\n\nTableÂ 2: Some regressions â€“ prettier\n\n\n\n\n\n\n\n\n\nDependent variable: Trade flow [â‚¬]\n\n\n\nRegression model\n\n\nOLS\nPoisson\n\n\n\n\nLog (distance [km])\n-47,643.02***\n-0.00***\n\n\n\n(10,905.57)\n(0.00)\n\n\nObservations\n38,325\n38,325\n\n\nRÂ²\n0.286\n0.751\n\n\nFE: Origin\nX\nX\n\n\nFE: Destination\nX\nX\n\n\nFE: Product\nX\nX\n\n\nFE: Year\nX\nX\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "blog/posts/scraping-wikipedia/index.html",
    "href": "blog/posts/scraping-wikipedia/index.html",
    "title": "Scraping wikipedia",
    "section": "",
    "text": "Every now and then, we want to get some data from the wild â€“ e.g., from Wikipedia articles. Weâ€™ll see two data types here:\nBefore we start, letâ€™s load the necessary libraries.\n# for scraping\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(parzer)\n\n# to parse it to spatial data\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(ggplot2)"
  },
  {
    "objectID": "blog/posts/scraping-wikipedia/index.html#parsing-to-sf",
    "href": "blog/posts/scraping-wikipedia/index.html#parsing-to-sf",
    "title": "Scraping wikipedia",
    "section": "Parsing to sf",
    "text": "Parsing to sf\nLastly, weâ€™ll want to parse this to an sf geometry. We can easily do this with its st_as_sf function.\n\ndistricts_sf &lt;- districts_with_coordinates |&gt; \n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "blog/posts/scraping-wikipedia/index.html#mapping-as-a-sanity-check",
    "href": "blog/posts/scraping-wikipedia/index.html#mapping-as-a-sanity-check",
    "title": "Scraping wikipedia",
    "section": "Mapping as a sanity check",
    "text": "Mapping as a sanity check\nJust to check if our two points are indeed within Afghanistan, weâ€™ll draw a map with them. For this, we first need to load the Afghanistan geometry.\n\nafghanistan &lt;- ne_countries(country = \"Afghanistan\", scale = \"small\", returnclass = \"sf\")\n\n\nggplot() + \n  geom_sf(data = afghanistan) + \n  geom_sf(data = districts_sf) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can see that the two points are within Afghanistan, so we seem to have mapped it correctly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sarah Listabarth",
    "section": "",
    "text": "Iâ€™m a PhD student at the Technical University of Dresden (TUD) in Germany, specializing in data science with a focus on data visualization. My academic work revolves around exploring and interpreting complex datasets to uncover new insights. I primarily code in R, a language and environment that I find well-suited for statistical analysis and graphical techniques in my research.\nI have a particular interest in the challenge of data wrangling â€“ organizing, cleaning, and transforming raw data into a usable format for analysis. This crucial step in the research process allows me to delve deep into the data, uncovering patterns and trends that might otherwise remain hidden.\nOne of my key strengths is in creating clear, compelling visuals to present my findings. I believe that good visualizations are essential for effectively communicating complex data insights."
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Software",
    "section": "",
    "text": "Provides a set of functions to implement donut analyses in R â€“\nfor when you're unsure which distance to a point of interest still counts as treated.\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n            \n                \n                     Documentation\n                \n            \n            \n        \n    \n    \n    \n        \n        Quarto template in TUD corporate design for presentations and extendes abstracts â€“\nfor both typst and latex output.\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n        \n    \n    \n    \n        \n        R script to analyze inconsistent writings in a word document, e.g. 'summariZe' vs. 'summariSe'.\nOutputs a word document that can be used with the FRedit macro (but should be reviewed first).\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n        \n    \n    \n    \n        \n        Word macros that editors can use to automate repetitive tasks.\n\n\n        \n            \n            \n            \n                \n                     Documentation\n                \n            \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "software/index.html#software",
    "href": "software/index.html#software",
    "title": "Software",
    "section": "",
    "text": "Provides a set of functions to implement donut analyses in R â€“\nfor when you're unsure which distance to a point of interest still counts as treated.\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n            \n                \n                     Documentation\n                \n            \n            \n        \n    \n    \n    \n        \n        Quarto template in TUD corporate design for presentations and extendes abstracts â€“\nfor both typst and latex output.\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n        \n    \n    \n    \n        \n        R script to analyze inconsistent writings in a word document, e.g. 'summariZe' vs. 'summariSe'.\nOutputs a word document that can be used with the FRedit macro (but should be reviewed first).\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n        \n    \n    \n    \n        \n        Word macros that editors can use to automate repetitive tasks.\n\n\n        \n            \n            \n            \n                \n                     Documentation\n                \n            \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "software/index.html#contributions",
    "href": "software/index.html#contributions",
    "title": "Software",
    "section": "Contributions",
    "text": "Contributions\n\n\n    \n    \n        \n            Base R \n        \n        Fixed a bug in `tools::toTitleCase()` where a letter following a hyphen and\na space would always be capitalized -- even when it should not\n\n\n        \n            \n            \n            \n                \n                     Documentation\n                \n            \n            \n        \n    \n    \n    \n        \n            ags package \n        \n        Fixed a bug â€“ a typo that led a reference into the void\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n        \n    \n    \n    \n        \n            modern2-cv template \n        \n        Added a template for the motivation letter that fits the CV template\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n        \n    \n    \n    \n        \n            purrr bookclub \n        \n        As part of the bookclub on the `purrr` package, added the corresponding chapter.\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n        \n    \n    \n    \n        \n            Effect bookclub \n        \n        As part of the bookclub on \"The Effect\" by Nick Huntington-Klein, added some chapters.\n\n\n        \n            \n            \n                \n                     GitHub\n                \n            \n            \n            \n            \n                \n                     Documentation\n                \n            \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "data-viz/index.html",
    "href": "data-viz/index.html",
    "title": "Data Visualizations",
    "section": "",
    "text": "Typewriter Chique: Saxony Switzerland \n          \n          Based on a post by Nicola Rennie, who did a typewriter-style map of Italy,\nI made this map of the Saxony Switzerland.\nIt's a region close to Dresden where I enjoy going for a hike. ðŸ¥¾\n\n          \n  \n          \n              \n              \n                  \n                       GitHub\n                  \n              \n              \n              \n          \n      \n      \n      \n          \n              Guest map \n          \n          People are coming from a couple of places for our wedding.\nI wanted to visualize where exactly, and which connections are especially strong.\n\n          \n  \n          \n              \n              \n                  \n                       GitHub\n                  \n              \n              \n              \n          \n      \n      \n      \n          \n              London marathon \n          \n          This map is part of the tidytuesday challenge.\nNicola Rennie cleaned a dataset on the London marathon, and I visualized \nwhere winners are from.\n\n          \n  \n          \n              \n              \n                  \n                       GitHub\n                  \n              \n              \n              \n          \n      \n      \n      \n          \n              Witches in the USA \n          \n          This map is part of the tidytuesday challenge.\nThis dataset was about haunted places in the USA.\n\n          \n  \n          \n              \n              \n                  \n                       GitHub\n                  \n              \n              \n              \n          \n      \n      \n      \n          \n              Rodents \n          \n          This map is part of the tidytuesday challenge.\nThe dataset was on rodents, and I focused on their diets.\n\n          \n  \n          \n              \n              \n                  \n                       GitHub\n                  \n              \n              \n              \n          \n      \n      \n      \n          \n              Using DHS data \n          \n          I work with DHS data in my research, so I wanted to show which countries are most eligible\nfor a panel data approach.\n\n          \n  \n          \n              \n              \n                  \n                       GitHub\n                  \n              \n              \n              \n          \n      \n      \n  \nNo matching items"
  },
  {
    "objectID": "blog/posts/osm-history-files/index.html",
    "href": "blog/posts/osm-history-files/index.html",
    "title": "Working with OSM history files in R",
    "section": "",
    "text": "Weâ€™re trying to find POIs throughout the history of OSM. Luckily, OSM does track when they are added. Only drawback? Weâ€™re working with huuuuuuge files. Letâ€™s see how we can do that.\nSo, what do we want to do? Extract\nfrom our OSM history file."
  },
  {
    "objectID": "blog/posts/osm-history-files/index.html#setting-things-up",
    "href": "blog/posts/osm-history-files/index.html#setting-things-up",
    "title": "Working with OSM history files in R",
    "section": "Setting things up",
    "text": "Setting things up\nLetâ€™s load the necessary libraries in R first.\n\nlibrary(duckdb)\nlibrary(ggplot2)\nlibrary(glue)\nlibrary(purrr)\nlibrary(terra)\nlibrary(tidyterra)\n\nThe file is huuuuge (around 140 GB), so we put it on an external drive. Iâ€™m guessing youâ€™ll do something similar. This is why weâ€™re declaring the data folder separately.\nTo work with osmium in this specific workflow, we need to note where osmium lives on our computer. So letâ€™s go ahead and define that as well.\nSo weâ€™ll have a set-up definition:\n\nwhere our data live (data_root)\nwhere our osmium lives (osmium)\n\n\ndata_root &lt;- \"E:\"\n\nosmium &lt;- \"C:/OSGeo4W/bin/osmium.exe\"\n\nNext, letâ€™s define the area in which we are searching for POIs. In our example, weâ€™re filtering for Togo.\n\nbbox_togo &lt;- c(\n  min_lon = -0.5,\n  min_lat = 5.5,\n  max_lon = 2,\n  max_lat = 11\n)"
  },
  {
    "objectID": "blog/posts/osm-history-files/index.html#extracting-the-area",
    "href": "blog/posts/osm-history-files/index.html#extracting-the-area",
    "title": "Working with OSM history files in R",
    "section": "Extracting the area",
    "text": "Extracting the area\nOsmium allows to extract OSM data by a given bounding box with the function extract. Osmium is a command line tool, so we need a special function within R to invoke commands: system2().\nThe following command\n\nextracts (line 4)\nfrom an OSM file (line 5) with history (line 6)\nwithin a bounding box (line 7)\nto an output file (line 8).\n\n\nsystem2(\n  osmium,\n  args = c(\n    \"extract\",\n    file.path(data_root, \"history-250623.osm.pbf\"),\n    \"--with-history\",\n    \"-b\", paste(bbox_togo, collapse = \",\"),\n    \"-o\", file.path(data_root, paste0(\"-history-250623\", paste(bbox_togo, collapse = \"_\"), \".osm.pbf\"))\n  )\n)"
  },
  {
    "objectID": "blog/posts/osm-history-files/index.html#lets-get-temporal",
    "href": "blog/posts/osm-history-files/index.html#lets-get-temporal",
    "title": "Working with OSM history files in R",
    "section": "Letâ€™s get temporal!",
    "text": "Letâ€™s get temporal!\nWeâ€™re switching to the next osmium tool: time-filter. This allows us to interact with the history part of the OSM history file: We can extract data for a specific point in time, or a time range (not covered here).\nWeâ€™re focusing on the years 2012â€“2025. time-filter needs the date time in a specific format. Weâ€™re building a helper function for this. Also, weâ€™re building a helper function to create descriptive names for the resulting files.\n\nyears &lt;- 2012:2025\n\n# time format for osmium\nget_timestamp &lt;- function(year) {\n  paste0(year, \"-01-01T00:00:00Z\")\n}\n\n# build file name based on year\nget_filename &lt;- function(year) {\n  file.path(data_root, paste0(\"togo-\", year, \".osm.pbf\"))\n}\n\nNext, weâ€™ll map over these years! Weâ€™re\n\nusing time-filter (line 7)\nto access the cropped history file (line 8)\nfor the given date time (line 9)\nand writing the corresponding file (lines 10â€“11)\nand getting the error messages in the R console (line 12).\n\nNow weâ€™ve got our data, cropped for the area of interest, sliced into years.\n\n\n\n\n\n\nNote\n\n\n\nThe way we used time-filter, it gives us the OSM file in its state on 1 January in each year.\n\n\n\n# Use walk2 to iterate over years and filter history file by date\nwalk(years, ~{\n  print(paste(\"extracting data for\", .x))\n  system2(\n    osmium,\n    args = c(\n      \"time-filter\",\n      file.path(data_root, \"history-250623-togo.osm.pbf\"),\n      get_timestamp(.x),                # date\n      \"-o\", get_filename(.x),           # output file name\n      \"--overwrite\" # output file overwrites existing file\n    ),\n    stderr = TRUE\n  )\n})"
  },
  {
    "objectID": "blog/posts/osm-history-files/index.html#filtering-the-pois-with-duckdb",
    "href": "blog/posts/osm-history-files/index.html#filtering-the-pois-with-duckdb",
    "title": "Working with OSM history files in R",
    "section": "Filtering the POIs with duckDB",
    "text": "Filtering the POIs with duckDB\nLetâ€™s find our POIs! For this, weâ€™re using duckDB, a light-weight data base. Again, we can call it from within R. It ships with the duckdb package. duckDB comes with extensions, including a spatial one. With this, we can handle spatial data and spatial formats, such as the .osm.pbf format.\n\n\n\n\n\n\nNote\n\n\n\nduckDB cannot handly OSM history files â€“ it just drops the time information. This is why we need to take a longer route: chopping up the history file first, then putting it back together with the year information.\n\n\nTo set it up, we first connect to the database. Weâ€™ll need to make sure we have the spatial extension installed and enabled.\n\n# setup duckdb instance\ncon &lt;- dbConnect(duckdb())\ndbExecute(con, \"INSTALL spatial;\")\ndbExecute(con, \"LOAD spatial;\")\n\nTo make subsequent queries easier, we create a so-called common table expression (CTE). You can imagine it like a data.frame in R. We add the information where the file comes from (here: the year) in one column. The process is a bit like applying bind_rows on a list of data.frames.\nThis is implemented with some SQL magic.\n\nqueries &lt;- map_chr(years, ~{\n  glue(\"SELECT *, '{.x}' AS year FROM '{get_filename(.x)}'\")\n})\ncte &lt;- paste(queries, collapse = \"\\nUNION ALL\\n\")\n\nNow, finally, we can query our historic data for our POIs! For that, weâ€™re selecting a couple of columns:\n\nid\ntag (for POI)\nlat, lon (geometry)\nyear\n\nand then filtering for\n\npoint geometries (node in OSM speech)\nrestaurants (amenity: restaurant in OSM speech).\n\nLastly, we parse it to a terra::vect object.\n\n# Use CTE and get all restaurants with the respective year\nres &lt;- dbGetQuery(\n  con,\n  glue(\n    \"WITH all_years AS ({cte})\n    SELECT \n      id,\n      tags,\n      lat,\n      lon,\n      year,\n    FROM all_years\n    WHERE\n      kind = 'node'\n      AND tags.amenity = 'restaurant';\n  \")\n) |&gt; vect(geom = c(\"lon\", \"lat\"), crs = \"EPSG:4326\")"
  },
  {
    "objectID": "blog/posts/reading-csv-with-2row-colnames/index.html",
    "href": "blog/posts/reading-csv-with-2row-colnames/index.html",
    "title": "Reading a csv file with 2-row column names",
    "section": "",
    "text": "I recently wanted to check out election results from the last German Bundestag election, and read them into R. The data was saved in a csv file â€“ so far, so good. But the column names were spread over two rows. ðŸ˜±\nThis means Iâ€™m losing valuable information if Iâ€™m only reading the first row of column names. Additionally, the types of the rows get messed up, since the second row of the column names is read as data â€“ specifically, character data.\nSo, I wanted to combine both rows of column names into one. Hereâ€™s how I did it in R."
  },
  {
    "objectID": "blog/posts/reading-csv-with-2row-colnames/index.html#reading-the-first-row",
    "href": "blog/posts/reading-csv-with-2row-colnames/index.html#reading-the-first-row",
    "title": "Reading a csv file with 2-row column names",
    "section": "Reading the first row",
    "text": "Reading the first row\nThereâ€™s probably many ways to do this, but I decided to go for the following approach:\n\nread in the csv file starting at the first column name row\nclean the column names\nstore the column names in a variable\n\nTo read in the first row of column names, we can use the read.csv2() function. Weâ€™ll skip the first three rows (which only contain metadata) and read in only one row, since we donâ€™t need the content for now. Then, weâ€™ll clean the column names with janitor::clean_names() to make sure that theyâ€™re formatted like we want them to. Lastly, Iâ€™m removing the numbers that are attached at the end since some of the variable names so far are not unique (since theyâ€™re still missing the second row).\n\ncolnames_row1 &lt;- read.csv2(temp_file, skip = 4, nrows = 1) |&gt;\njanitor::clean_names() |&gt; \n  colnames() |&gt; \n  gsub(\"_[[:digit:]].*\", \"\", x = _)"
  },
  {
    "objectID": "blog/posts/reading-csv-with-2row-colnames/index.html#reading-both-column-names",
    "href": "blog/posts/reading-csv-with-2row-colnames/index.html#reading-both-column-names",
    "title": "Reading a csv file with 2-row column names",
    "section": "Reading both column names",
    "text": "Reading both column names\nWe could repeat this procedure for the second row of column names. However, itâ€™s better to create a function for this case. We simply take the pipeline from before and put it into a function.\n\nread_colnames &lt;- function(path, skip_rows = 4, no_rows = 1) {\n  path |&gt; \n    read.csv2(skip = skip_rows, nrows = no_rows) |&gt; \n    janitor::clean_names() |&gt; \n    colnames() |&gt; \n    gsub(\"_[[:digit:]].*\", \"\", x = _)\n}"
  },
  {
    "objectID": "blog/posts/sending-automated-emails/index.html",
    "href": "blog/posts/sending-automated-emails/index.html",
    "title": "Sending automated emails",
    "section": "",
    "text": "I recently needed to send out an email to many recipients, with only slight changes in the text. And I figured â€“ this must be something we can do with R, right?\nLuckily enough, thereâ€™s the RDCOMClient library which lets R access some programs, including Outlook. So if you have Outlook set up on your computer, this approach will work like a charm. Actually, it was kind of scary for me: this really works, and real emails get sent. ðŸ˜±"
  },
  {
    "objectID": "blog/posts/sending-automated-emails/index.html#preparation",
    "href": "blog/posts/sending-automated-emails/index.html#preparation",
    "title": "Sending automated emails",
    "section": "Preparation",
    "text": "Preparation\nWhat do we need?\n\nthe RDCOMClient library\na data.frame including the email addresses and the bit we want changed\na template text\n\nSo letâ€™s start by loading the needed libraries. We already talked about RDCOMClient. dplyr and glue make pasting our template and our information together easier, and purrr will allow us to loop over our info data.frame.\n\nlibrary(glue)\nlibrary(dplyr)\nlibrary(RDCOMClient)\nlibrary(purrr)\n\nLetâ€™s start with our template! Weâ€™ll make it nice and short.\n\ntemplate &lt;- \"Good morning!\nDo you enjoy {food}?\nHave a great day.\"\n\nNext, weâ€™d load our data.frame; for this showcase purpose, weâ€™ll just make it up from scratch. Weâ€™ll combine our template and our bit of information into a new column, text.\n\ninfo &lt;- tibble::tribble(\n  ~email, ~info,\n  \"anna@example.com\", \"apples\",\n  \"bernardo@example.com\", \"bananas\",\n  \"charlie@example.com\", \"chocolate\",\n  \"donald@example.com\", \"donuts\"\n) |&gt; \n  dplyr::mutate(text = glue::glue(template, food = info))\n\n\ninfo |&gt; kableExtra::kbl()\n\n\n\n\nemail\ninfo\ntext\n\n\n\n\nanna@example.com\napples\nGood morning! Do you enjoy apples? Have a great day.\n\n\nbernardo@example.com\nbananas\nGood morning! Do you enjoy bananas? Have a great day.\n\n\ncharlie@example.com\nchocolate\nGood morning! Do you enjoy chocolate? Have a great day.\n\n\ndonald@example.com\ndonuts\nGood morning! Do you enjoy donuts? Have a great day."
  },
  {
    "objectID": "blog/posts/sending-automated-emails/index.html#mail-function",
    "href": "blog/posts/sending-automated-emails/index.html#mail-function",
    "title": "Sending automated emails",
    "section": "Mail function",
    "text": "Mail function\nNext, letâ€™s set up the function which we will map over to send our mails! Letâ€™s start by understanding what RDCOMClient does for us.\n\nOpen Outlook\nThe first thing we want to do â€“ and just once â€“ is to open the Outlook app.\n\nOutlook &lt;- COMCreate(\"Outlook.Application\")\n\n\n\nCreate an email\nWe need to start off by creating an email â€“ just like when youâ€™re in Outlook, you click on Create. Weâ€™re telling RDCOMClient that itâ€™s Outlook we want to use by putting it first, and we put the command we want executed after the $ sign.\nLetâ€™s also save this email to an object, so that we can work on it again after.\n\nEmail &lt;- Outlook$CreateItem(0)\n\n\n\nSet recipient, subject and body\nWhatâ€™s the next thing youâ€™d do when writing an email? Youâ€™d type in who you want to send the email to and what itâ€™s about â€“ recipient and subject. Then, youâ€™d enter the text â€“ the body.\nWe can do that using attributes of the Email object weâ€™ve created.\n\nfirst_info &lt;- info |&gt; head(1)\n\nEmail[[\"to\"]] &lt;- first_info$email\nEmail[[\"subject\"]] &lt;- \"Hello!\"\nEmail[[\"body\"]] &lt;- first_info$text\n\n\n\nSending the email\nLastly, we need to click on Send. This is, again, an Outlook$ function, not an attribute.\n\nEmail$Send()\n\n\n\nPiecing together the pieces\nThis is our final function, where we have incorporated everything we need. Note that we donâ€™t include opening Outlook â€“ this is something we only need to do once. We can now take this function and map over it.\n\n\n\n\n\n\nTip\n\n\n\nNote that in our approach, weâ€™re using the same subject for all emails. You could, however, also map over this part.\n\n\n\nsend_mail &lt;- function(to = \"\",\n                      body = \"\",\n                      subject = \"Hello from R\"){\n  Email = Outlook$CreateItem(0)\n  Email[[\"to\"]] = to\n  Email[[\"subject\"]] = subject\n  Email[[\"body\"]] = body\n  Email$Send()\n}"
  },
  {
    "objectID": "blog/posts/sending-automated-emails/index.html#mapping-over-the-function",
    "href": "blog/posts/sending-automated-emails/index.html#mapping-over-the-function",
    "title": "Sending automated emails",
    "section": "Mapping over the function",
    "text": "Mapping over the function\nLetâ€™s now take purr to map over this! Weâ€™ll need the walk2 function, since we have two inputs for every mail that change:\n\nemail\nbody/text\n\n\nOutlook &lt;- COMCreate(\"Outlook.Application\")\nwalk2(.x = info$email,\n      .y = info$text,\n      .f = ~ send_mail(to = .x, body = .y))\n\nAnd there we have it â€“ we can send out an email to Anna, Bernardo, Charlie and Donald all at once! You can check in your Outlook outbox that these emails actually sent."
  },
  {
    "objectID": "blog/posts/sending-automated-emails/index.html#approaches-without-outlook",
    "href": "blog/posts/sending-automated-emails/index.html#approaches-without-outlook",
    "title": "Sending automated emails",
    "section": "Approaches without Outlook",
    "text": "Approaches without Outlook\nUsing RDCOMClient to access Outlook is a bit hacky, but was exactly the right thing for me. However, we could use a more direct approach, without using the Outlook user interface.\nThis blogpost by mailtrap goes into detail about how you can achieve this."
  },
  {
    "objectID": "blog/posts/opening-kmz-files/index.html",
    "href": "blog/posts/opening-kmz-files/index.html",
    "title": "Opening .kmz files",
    "section": "",
    "text": "When working with spatial data, one thing Iâ€™ve noticed is: thereâ€™s a lot of obscure data formats. And with every one of them, it takes me a while to figure out how to handle them in R. So I decided to make a series of blog posts on how to handle some of the spatial formats Iâ€™ve come across.\nThis post is on .kmz files. If youâ€™re working with Google Earth Pro, this is the format your vector files will be exported in. Recently, Iâ€™ve worked quite a bit with Google Earth Pro to collect data on landfills by hand. Youâ€™ll find an example .kmz file here."
  },
  {
    "objectID": "blog/posts/opening-kmz-files/index.html#extracting-information-from-the-name",
    "href": "blog/posts/opening-kmz-files/index.html#extracting-information-from-the-name",
    "title": "Opening .kmz files",
    "section": "Extracting information from the Name",
    "text": "Extracting information from the Name\nWhen we check out our agbogbloshie_polygon data frame, we can see that thereâ€™s information contained in the Name column, specifically: the year for which the polygon shape is valid. So letâ€™s extract that numeric information from the character column! Because the description is quite long and has much information included â€“ and itâ€™s always formatted the same way â€“, weâ€™ll use the unglue package to extract the information. Also, the landfill is misspelt in some cases, so we need to account for that.\n\nagbogbloshie_polygon &lt;- agbogbloshie_polygon |&gt; \n  unglue_unnest(Name, \n                patterns = c(\"{landfill_name}_{month}_{year}_{polygon_no}\",\n                             \"{landfill_name}_{month}_{year}\",\n                             \"{landfill_name} {month}_{year}\"),\n                remove = FALSE) |&gt; \n  mutate(landfill_name = case_match(landfill_name,\n                                    \"Agblogbloshie\" ~ \"Agbogbloshie\",\n                                    \"Agbogbgloshie\" ~ \"Agbogbloshie\",\n                                    .default = \"Agbogbloshie\"))\n\nagbogbloshie_polygon\n\nSimple feature collection with 8 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XYZ\nBounding box:  xmin: -0.2311228 ymin: 5.54208 xmax: -0.2246721 ymax: 5.556922\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n# A tibble: 8 Ã— 7\n  Name                     Description landfill_name month year  polygon_no\n* &lt;chr&gt;                    &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n1 Agbogbloshie_02_2020     \"\"          Agbogbloshie  02    2020  &lt;NA&gt;      \n2 Agbogbloshie_10_2018     \"\"          Agbogbloshie  10    2018  &lt;NA&gt;      \n3 Agblogbloshie 10_2016    \"\"          Agbogbloshie  10    2016  &lt;NA&gt;      \n4 Agbogbloshie_10_2018_02  \"\"          Agbogbloshie  10    2018  02        \n5 Agbogbloshie_10_2018_03  \"\"          Agbogbloshie  10    2018  03        \n6 Agbogbgloshie_02_2020_02 \"\"          Agbogbloshie  02    2020  02        \n7 Agbogbloshie_02_2020_03  \"\"          Agbogbloshie  02    2020  03        \n8 Agbogbloshie_10_2016_02  \"\"          Agbogbloshie  10    2016  02        \n# â„¹ 1 more variable: geometry &lt;MULTIPOLYGON [Â°]&gt;\n\n\nLetâ€™s use this information to plot the landfill in the two different years! Seems like the landfill grew quite a bit in between.\n\nggplot() +\n  geom_sf(data = agbogbloshie_polygon) +\n  facet_wrap(~year) +\n  geom_sf(data = agbogbloshie_point) +\n  theme_void() +\n  ggspatial::annotation_scale()\n\n\n\n\n\n\n\nFigureÂ 1: Agbogbloshie landfill over time\n\n\n\n\n\nThis looks good! However, we can see that the landfills consist of multiple polygons. Letâ€™s get them together."
  },
  {
    "objectID": "blog/posts/opening-kmz-files/index.html#summarizing-into-one-polygon",
    "href": "blog/posts/opening-kmz-files/index.html#summarizing-into-one-polygon",
    "title": "Opening .kmz files",
    "section": "Summarizing into one polygon",
    "text": "Summarizing into one polygon\nIn every year, we have more than one polygon. We want to collapse these into a single multipolygon. For that, we need to summarize them by year and landfill.\n\nagbogbloshie_polygon &lt;- agbogbloshie_polygon |&gt; \n  st_zm() |&gt; \n  st_transform(crs = \"ESRI:54009\") |&gt; \n  st_make_valid() |&gt; \n  # summarize geometry into multipolygons\n  group_by(landfill_name, year, month) |&gt; \n  summarize() |&gt; \n  ungroup()"
  },
  {
    "objectID": "blog/posts/opening-kmz-files/index.html#calculating-area",
    "href": "blog/posts/opening-kmz-files/index.html#calculating-area",
    "title": "Opening .kmz files",
    "section": "Calculating area",
    "text": "Calculating area\nAs a next step, letâ€™s calculate the area of these polygons. After all, itâ€™s interesting to see if this landfill shrinks or grows!\nFor now, our data are unprojected, though. We can check this by getting the coordinate reference system (CRS) of our data. The output is very long and includes a lot of information, but we only need to focus on the first line, the User input. The next lines show the corresponding well-known text (wkt), which details exactly how the CRS works. We can see here that the CRS is still WGS 84, which means â€“ unprojected.\n\nst_crs(agbogbloshie)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nHowever, we know that the world is shaped like a potato, and that these unprojected data are biased. To correct for this, we first need to project the data into a CRS thatâ€™s accurate for that world region when it comes to area â€“ we also call this area-preserving projection. The official CRS for Ghana, where this landfill is located, has the EPSG code 2136. Once weâ€™ve projected the polygons, we can easily calculate the area with the sf package.\n\nagbogbloshie_polygon &lt;- agbogbloshie_polygon |&gt; \n  st_transform(crs = \"epsg:2136\") |&gt; \n  mutate(area = st_area(geometry)) \n\nagbogbloshie_polygon\n\nSimple feature collection with 3 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1179342 ymin: 316685.8 xmax: 1181692 ymax: 322068\nProjected CRS: Accra / Ghana National Grid\n# A tibble: 3 Ã— 5\n  landfill_name year  month                                      geometry   area\n* &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;                                &lt;MULTIPOLYGON&gt;  [m^2]\n1 Agbogbloshie  2016  10    (((1180085 320017.8, 1180468 319802.8, 11805â€¦ 2.41e5\n2 Agbogbloshie  2018  10    (((1180357 319898.9, 1180372 319891.8, 11803â€¦ 2.94e5\n3 Agbogbloshie  2020  02    (((1179921 320055.5, 1179936 320041.3, 11799â€¦ 2.56e5\n\n\nYouâ€™ll notice that the area column has a specific format: itâ€™s a unit. This package makes it easy to convert values from one unit into another. Letâ€™s see how we can convert the area into hectares. Afterwards, letâ€™s drop the unit because we have the data in the final shape we want them.\n\nagbogbloshie_polygon &lt;- agbogbloshie_polygon |&gt; \n  mutate(area_ha = set_units(area, \"ha\") |&gt; drop_units())\n\nAnd there we have it, a data frame with the attributes that we need! Letâ€™s check out the area in comparison. We see that thereâ€™s not that much difference!\n\nggplot(data = agbogbloshie_polygon) +\n  geom_col(aes(x = year |&gt; as.character(), y = area_ha)) +\n  coord_flip() +\n  labs(y = \"Area (ha)\",\n       x = \"\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigureÂ 2: Agbogbloshie landfill area over time"
  },
  {
    "objectID": "blog/posts/locator-map/index.html",
    "href": "blog/posts/locator-map/index.html",
    "title": "Creating a locator map with ggplot2",
    "section": "",
    "text": "Sometimes I visualize data on a country that not everyone is familiar with. In this case, a locator map can be helpful: It shows where in the world my country of interest is. Here, Iâ€™ll show you how to create such a locator map for Ghana."
  },
  {
    "objectID": "blog/posts/locator-map/index.html#plotting-the-graticule",
    "href": "blog/posts/locator-map/index.html#plotting-the-graticule",
    "title": "Creating a locator map with ggplot2",
    "section": "Plotting the graticule",
    "text": "Plotting the graticule\nLetâ€™s start out with just plotting our graticule. Thatâ€™s the net of lines that make up the latitudes and longitudes, and that weâ€™re used to seeing from a gobe. For that, weâ€™ll need:\n\na graticule\nthe projection\na theme_void()\n\n\n\n\n\n\n\nTip\n\n\n\nTechnically, you can also simply use the ggplot gridlines. However, theyâ€™re a bit buggy, so Iâ€™d recommend using these explicit gridlines. Thatâ€™s why weâ€™re also using the theme_void(): so we donâ€™t get those gridlines as well.\n\n\nWe can use the function sf::st_graticule to create this graticule on the fly.\n\nglobe &lt;- ggplot() +\n  # define our graticules\n  geom_sf(data = st_graticule(n_discr = 1),\n          col = \"grey80\", fill = \"white\", linewidth = .25) +\n  # define the CRS and theme\n  coord_sf(crs = locator_crs) +\n  theme_void() \n\nglobe\n\n\n\n\n\n\n\n\nAnd ta-da, thatâ€™s our globe!"
  },
  {
    "objectID": "blog/posts/locator-map/index.html#plotting-the-world",
    "href": "blog/posts/locator-map/index.html#plotting-the-world",
    "title": "Creating a locator map with ggplot2",
    "section": "Plotting the world",
    "text": "Plotting the world\nLetâ€™s continue with plotting the world on top, so we have some context for locating. Weâ€™ll plot the world in a grey tone, since we only need it for context, but donâ€™t want the map reader to be overwhelmed.\nWeâ€™ll need:\n\na world sf dataset\nour CRS (again)\n\nSince weâ€™re using the Natural Earth dataset, weâ€™ll give credit to them through the caption.\n\n\n\n\n\n\nTip\n\n\n\nWe need to define the CRS again because weâ€™re calling a geometry (geom_sf()) after we last defined the CRS.\n\n\n\nlocator_map &lt;- globe +\n  geom_sf(data = world,\n          col = \"grey85\") +\n  # define the CRS\n  coord_sf(crs = locator_crs) +\n  # give credit\n  labs(caption = \"Data: Natural Earth\")\n\nlocator_map"
  },
  {
    "objectID": "blog/posts/locator-map/index.html#highlighting-our-country",
    "href": "blog/posts/locator-map/index.html#highlighting-our-country",
    "title": "Creating a locator map with ggplot2",
    "section": "Highlighting our country",
    "text": "Highlighting our country\nNow letâ€™s get to the interesting part: highlighting!\n\nHighlighting by color\nWith countries that are large enough, simply highlighting by color works well. We give the country any other color than grey, and itâ€™s going to pop.\nSo what do we need?\n\nour countryâ€™s geometry\na highlighting color (Iâ€™m choosing white)\nour CRS (again)\n\nWeâ€™re starting with our last map, and add just our country on top.\n\nlocator_map +\n  # define our country\n  geom_sf(data = world |&gt; filter(sovereignt == \"Ghana\"),\n          fill = \"#1EB53A\",\n          col = \"grey85\") +\n  # make sure we have the correct CRS\n  coord_sf(crs = locator_crs) \n\n\n\n\n\n\n\n\n\n\nHighlighting with a dot\nSometimes, we donâ€™t want to highlight a whole country, but instead just a smaller spot â€“ maybe\n\na city,\na mountain or\na tree that we find interesting.\n\nIn this case, it makes sense to use a point geometry for highlighting. So weâ€™ll need\n\na point geometry\nour CRS (again)\n\n\n# make Ghana a point geometry\nghana_dot &lt;- world |&gt; \n  filter(sovereignt == \"Ghana\") |&gt; \n  st_centroid()\n\nlocator_map +\n  geom_sf(data = ghana_dot) +\n  # make sure we have the correct CRS\n  coord_sf(crs = locator_crs) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nHere, we donâ€™t have to define a color â€“ having one point already pops.\n\n\n\n\nHighlighting with a bounding box\nSometimes, we want to highlight a larger area that wouldnâ€™t show well on a globe: maybe an island group or a small country. In this case, we can also highlight using a bounding box of our original geometry.\nSo letâ€™s start by creating our bounding box for the geometry. Importantly, it needs to be a polygon so we can plot it.\n\n\n\n\n\n\nTip\n\n\n\nMake sure you transform your polygonâ€™s CRS to the one youâ€™ll be using for the locator map. Otherwise, the bounding box wonâ€™t be rectangular.\n\n\n\nghana_bbox &lt;- world |&gt; \n  filter(sovereignt == \"Ghana\") |&gt; \n  st_transform(locator_crs) |&gt; \n  st_bbox() |&gt; \n  st_as_sfc()\n\nggplot() +\n  geom_sf(data = ghana_bbox) +\n  coord_sf(crs = locator_crs)\n\n\n\n\n\n\n\n\nIf we plot it, we can see itâ€™s just a rectangle. And itâ€™s got 90Â° angles when using our locator map CRS. Perfect, so letâ€™s plot it on top of our raw locator map! Letâ€™s make sure that we buffer the bounding box a bit â€“ that way, we can see the borders of our country. Setting fill to NA makes sure that we can see whatâ€™s below the bounding box.\n\nlocator_map +\n  geom_sf(data = ghana_bbox |&gt; st_buffer(5e4),\n          fill = NA,\n          lwd = .5) +\n  # make sure we have the correct CRS\n  coord_sf(crs = locator_crs) \n\n\n\n\n\n\n\n\nAnd there we go: three different ways of creating a locator map. If you want to combine it with your more detailed map into a single plot, Iâ€™d recommend the package patchwork for this."
  }
]